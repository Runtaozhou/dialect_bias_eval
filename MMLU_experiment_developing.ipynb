{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/uar6nw/.local/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.1)\n",
      "Requirement already satisfied: jinja2 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (74.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/uar6nw/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/uar6nw/.local/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/uar6nw/.local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /home/uar6nw/.local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/uar6nw/.local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/uar6nw/.local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/uar6nw/.local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/uar6nw/.local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/uar6nw/.local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/uar6nw/.local/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/uar6nw/.local/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/uar6nw/.local/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/uar6nw/.local/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/uar6nw/.local/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /home/uar6nw/.local/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /home/uar6nw/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/uar6nw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from getDialect import detectDialect\n",
    "from qna_simulation import run_simulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A simulation (one question per subject from MMLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## loading dataset names from pickle file\n",
    "with open(\"dataset_name.pkl\", \"rb\") as f:\n",
    "    dataset_names = pickle.load(f)\n",
    "\n",
    "'''\n",
    "important note:\n",
    "parameter \"aave\" = True means you want to change the whole question prompt to AAVE \n",
    "However, the \"aave_instruct\" = True just means you only want to change the instruction part of the question prompt to AAVE, \n",
    "the acutal question remains SAE. \"aave\" and \"aave_instruct\" can not both be True.\n",
    "'''\n",
    "\n",
    "df_regular = run_simulation(dataset_names = dataset_names, model = \"gpt-3.5\", aave= False, n_run = 1, aave_instruct = False, converter_type = \"both\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the Q&A simulation dataset from 7 different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "gpt3_path = \"qna_dataset/gpt3/\"\n",
    "gpt4_path = \"qna_dataset/gpt4/\"\n",
    "llama31_path = \"qna_dataset/llama3.1/\"\n",
    "qwen_path = \"qna_dataset/qwen2.5/\"\n",
    "llama32_path = \"qna_dataset/llama3.2/\"\n",
    "gemma_path = \"qna_dataset/gemma2/\"\n",
    "mistral_path = \"qna_dataset/mistral/\"\n",
    "\n",
    "df_regular_gpt3 = pd.read_csv(gpt3_path+'regular_mmlu_qna.csv')\n",
    "df_phonate_gpt3 = pd.read_csv(gpt3_path+'aave_phonate_mmlu_qna.csv')\n",
    "df_llm_gpt3 = pd.read_csv(gpt3_path+'aave_llm_mmlu_qna.csv')\n",
    "df_multivalue_gpt3 =  pd.read_csv(gpt3_path+'aave_multi_value_mmlu_qna.csv')\n",
    "df_multi_phonate_gpt3 = pd.read_csv(gpt3_path+'aave_multi_phonate_mmlu_qna.csv')\n",
    "\n",
    "df_regular_gpt4 = pd.read_csv(gpt4_path+'regular_mmlu_qna.csv')\n",
    "df_phonate_gpt4 = pd.read_csv(gpt4_path+'aave_phonate_mmlu_qna.csv')\n",
    "df_multivalue_gpt4 = pd.read_csv(gpt4_path+'aave_multi_value_mmlu_qna.csv')\n",
    "df_llm_gpt4 = pd.read_csv(gpt4_path+'aave_llm_mmlu_qna.csv')\n",
    "df_multi_phonate_gpt4 = pd.read_csv(gpt4_path+'aave_multi_phonate_mmlu_qna.csv')\n",
    "\n",
    "df_regular_llama31 = pd.read_csv(llama31_path + \"regular_mmlu_qna.csv\")\n",
    "df_phonate_llama31 = pd.read_csv(llama31_path + \"aave_phonate_mmlu_qna.csv\")\n",
    "df_llm_llama31 = pd.read_csv(llama31_path + \"aave_llm_mmlu_qna.csv\")\n",
    "df_multi_phonate_llama31 = pd.read_csv(llama31_path + \"aave_multi_phonate_mmlu_qna.csv\")\n",
    "df_multivalue_llama31 = pd.read_csv(llama31_path + \"aave_multi_value_mmlu_qna.csv\")\n",
    "\n",
    "df_regular_llama32 = pd.read_csv(llama32_path + \"regular_mmlu_qna.csv\")\n",
    "df_phonate_llama32 = pd.read_csv(llama32_path + \"aave_phonate_mmlu_qna.csv\")\n",
    "df_llm_llama32 = pd.read_csv(llama32_path + \"aave_llm_mmlu_qna.csv\")\n",
    "df_multi_phonate_llama32 = pd.read_csv(llama32_path + \"aave_multi_phonate_mmlu_qna.csv\")\n",
    "df_multivalue_llama32 = pd.read_csv(llama32_path + \"aave_multi_value_mmlu_qna.csv\")\n",
    "\n",
    "df_regular_qwen = pd.read_csv(qwen_path + \"regular_mmlu_qna.csv\")\n",
    "df_phonate_qwen = pd.read_csv(qwen_path + \"aave_phonate_mmlu_qna.csv\")\n",
    "df_llm_qwen = pd.read_csv(qwen_path + \"aave_llm_mmlu_qna.csv\")\n",
    "df_multi_phonate_qwen = pd.read_csv(qwen_path + \"aave_multi_phonate_mmlu_qna.csv\")\n",
    "df_multivalue_qwen = pd.read_csv(qwen_path + \"aave_multi_value_mmlu_qna.csv\")\n",
    "\n",
    "df_regular_gemma2 = pd.read_csv(gemma_path + \"regular_mmlu_qna.csv\")\n",
    "df_phonate_gemma2 = pd.read_csv(gemma_path + \"aave_phonate_mmlu_qna.csv\")\n",
    "df_llm_gemma2 = pd.read_csv(gemma_path + \"aave_llm_mmlu_qna.csv\")\n",
    "df_multi_phonate_gemma2 = pd.read_csv(gemma_path + \"aave_multi_phonate_mmlu_qna.csv\")\n",
    "df_multivalue_gemma2 = pd.read_csv(gemma_path + \"aave_multi_value_mmlu_qna.csv\")\n",
    "\n",
    "df_regular_mistral = pd.read_csv(mistral_path + \"regular_mmlu_qna.csv\")\n",
    "df_phonate_mistral = pd.read_csv(mistral_path + \"aave_phonate_mmlu_qna.csv\")\n",
    "df_llm_mistral = pd.read_csv(mistral_path + \"aave_llm_mmlu_qna.csv\")\n",
    "df_multi_phonate_mistral = pd.read_csv(mistral_path + \"aave_multi_phonate_mmlu_qna.csv\")\n",
    "df_multivalue_mistral = pd.read_csv(mistral_path + \"aave_multi_value_mmlu_qna.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of sae question from mistral is: 0.5284210526315789\n",
      "the accuracy of aave phonate question from mistral is: 0.44\n",
      "the accuracy of aave llm question from mistral is: 0.4726315789473684\n",
      "the accuracy of aave multivalue question from mistral is: 0.46\n",
      "the accuracy of aave multivalue + phonate question from mistral is: 0.5059649122807017\n"
     ]
    }
   ],
   "source": [
    "from utils import extract_model_accuracy\n",
    "matches_regular_mistral = extract_model_accuracy(df_regular_mistral)\n",
    "matches_phonate_mistral = extract_model_accuracy(df_phonate_mistral)\n",
    "matches_llm_mistral =  extract_model_accuracy(df_llm_mistral)\n",
    "matches_multi_value_mistral =  extract_model_accuracy(df_multi_phonate_mistral)\n",
    "matches_multi_phonate_mistral =  extract_model_accuracy(df_multivalue_mistral)\n",
    "\n",
    "print(f\"the accuracy of sae question from mistral is: {matches_regular_mistral}\")\n",
    "print(f\"the accuracy of aave phonate question from mistral is: {matches_phonate_mistral}\")\n",
    "print(f\"the accuracy of aave llm question from mistral is: {matches_llm_mistral}\")\n",
    "print(f\"the accuracy of aave multivalue question from mistral is: {matches_multi_value_mistral}\")\n",
    "print(f\"the accuracy of aave multivalue + phonate question from mistral is: {matches_multi_phonate_mistral}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from random import sample \n",
    "from sklearn import svm\n",
    "import convokit\n",
    "from collections import defaultdict\n",
    "from  readability.readability import Readability\n",
    "from convokit import Corpus, Speaker, Utterance\n",
    "from convokit import download\n",
    "from convokit import Classifier\n",
    "from convokit import PolitenessStrategies\n",
    "from pandas import DataFrame\n",
    "from typing import List, Dict, Set\n",
    "from convokit import TextParser\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "random.seed(10)\n",
    "def create_classifier():\n",
    "    random.seed(10)\n",
    "    parser = TextParser(verbosity=1000)\n",
    "    ps = PolitenessStrategies()\n",
    "    wiki_corpus = Corpus(filename=download(\"wikipedia-politeness-corpus\"))\n",
    "    utterance_list = [wiki_corpus.get_utterance(i) for i in wiki_corpus.get_utterance_ids()]\n",
    "    stack_corpus = Corpus(filename=download(\"stack-exchange-politeness-corpus\"))\n",
    "    combine_corpus = stack_corpus.add_utterances(utterance_list)\n",
    "    neutral_utterances = [utt for utt in combine_corpus.iter_utterances() if utt.meta[\"Binary\"] == 0]\n",
    "    polite_utterances = [utt for utt in combine_corpus.iter_utterances() if utt.meta[\"Binary\"] == 1]\n",
    "    sampled_neutral1 = sample(neutral_utterances,len(polite_utterances))\n",
    "    sampled_neutral1.extend(polite_utterances)\n",
    "    combine_corpus_polite = Corpus(utterances=sampled_neutral1)\n",
    "    combine_corpus_polite = parser.transform(combine_corpus_polite)\n",
    "    combine_corpus_polite = ps.transform(combine_corpus_polite, markers=True)\n",
    "    labeller_polite = lambda utt: 0 if utt.meta['Binary'] == 0 else 1\n",
    "    clf_polite = Classifier(obj_type=\"utterance\", \n",
    "                            pred_feats=[\"politeness_strategies\"], \n",
    "                            labeller=labeller_polite,\n",
    "                            )\n",
    "    clf_polite.fit(combine_corpus_polite)\n",
    "    return clf_polite\n",
    "\n",
    "def build_corpus(df, text_attribute : str = \"\"):\n",
    "    speaker = Speaker()\n",
    "    utter_lst = []\n",
    "    for i in range(len(df)):\n",
    "        utter = Utterance(speaker =speaker, text = df.loc[i][text_attribute], id = str(i))\n",
    "        utter_lst.append(utter)\n",
    "    corpus = Corpus(utterances =utter_lst)\n",
    "    test_ids = corpus.get_utterance_ids()\n",
    "    corpus = parser.transform(corpus)\n",
    "    corpus = ps.transform(corpus, markers=True)\n",
    "    return corpus\n",
    "    \n",
    "def politeness_prediction(clf_polite, corpus):\n",
    "    print('--------prediciting politeness-----------')\n",
    "    pred = clf_polite.transform(corpus)\n",
    "    df_pred_polite = clf_polite.summarize(pred)\n",
    "    polite_num = sum(df_pred_polite['prediction'])\n",
    "    # creating df with neutral utterance\n",
    "    df_pred_nuetral = df_pred_polite[df_pred_polite['prediction'] == 0]\n",
    "    nuetral_num = len(df_pred_nuetral)\n",
    "    return [polite_num, nuetral_num]\n",
    "\n",
    "\n",
    "def get_flesch_score(df, model:str, conversion_type:str):\n",
    "    score =  []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        try:\n",
    "            r = Readability(df['answer'][i]).flesch()\n",
    "            score.append(r.score) \n",
    "        except:\n",
    "            continue\n",
    "    with open(f'pickle/{model}/flesch_score/{conversion_type}_score.pkl', 'wb') as f:\n",
    "        pickle.dump(score, f)\n",
    "    return score\n",
    "\n",
    "# def get_length(df):\n",
    "#     text_len = []\n",
    "#     for i in tqdm(range(len(df))):\n",
    "#         text_len. append(len(df['answer'][i].split()))\n",
    "#     return text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = TextParser(verbosity=1000)\n",
    "ps = PolitenessStrategies()\n",
    "\n",
    "corpus_mistral = build_corpus(df_regular_mistral, 'answer')\n",
    "corpus_aave_mistral  = build_corpus(df_phonate_mistral, 'answer')\n",
    "corpus_aave_llm_mistral= build_corpus(df_llm_mistral, 'answer')\n",
    "corpus_aave_multi_mistral  = build_corpus(df_multivalue_mistral, 'answer')\n",
    "corpus_aave_phonate_multi_mistral = build_corpus(df_multi_phonate_mistral, 'answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politeness_score = pd.read_csv('politeness_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_polite = create_classifier()\n",
    "\n",
    "score_list_sae_gemma= politeness_prediction(clf_polite, corpus_mistral)\n",
    "print(f'politeness score sae: {score_list_sae_gemma}')\n",
    "title = [\"mistral\", \"none\"]\n",
    "title.extend(score_list_sae_gemma)\n",
    "df_politeness_score.loc[len(df_politeness_score)] = title\n",
    "\n",
    "\n",
    "score_list_phonate_gemma = politeness_prediction(clf_polite, corpus_aave_mistral)\n",
    "print(f'politeness score phonate: {score_list_phonate_gemma}')\n",
    "title = [\"mistral\", \"phonate\"]\n",
    "title.extend(score_list_phonate_gemma)\n",
    "df_politeness_score.loc[len(df_politeness_score)] = title\n",
    "\n",
    "\n",
    "score_list_llm_gemma = politeness_prediction(clf_polite, corpus_aave_llm_mistral)\n",
    "print(f'politeness score llm: {score_list_llm_gemma}')\n",
    "title = [\"mistral\", \"llm\"]\n",
    "title.extend(score_list_llm_gemma)\n",
    "df_politeness_score.loc[len(df_politeness_score)] = title\n",
    "\n",
    "\n",
    "score_list_multi_gemma = politeness_prediction(clf_polite, corpus_aave_multi_mistral)\n",
    "print(f'politeness score multivalue: {score_list_multi_gemma}')\n",
    "title = [\"mistral\", \"multivalue\"]\n",
    "title.extend(score_list_multi_gemma)\n",
    "df_politeness_score.loc[len(df_politeness_score)] = title\n",
    "\n",
    "\n",
    "score_list_phonate_multi_gemma = politeness_prediction(clf_polite, corpus_aave_phonate_multi_mistral)\n",
    "print(f'politeness score multi phonate: {score_list_phonate_multi_gemma}')\n",
    "title = [\"mistral\", \"phonate+multivalue\"]\n",
    "title.extend(score_list_phonate_multi_gemma)\n",
    "df_politeness_score.loc[len(df_politeness_score)] =title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politeness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_politeness_score.to_csv('politeness_score.csv',header= True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "reg_flesh_score_mistral = get_flesch_score(df_regular_mistral, \"mistral\", \"sae\")\n",
    "phonate_flesch_score_mistral = get_flesch_score(df_phonate_mistral, \"mistral\", \"aave_phonate\")\n",
    "llm_flesch_score_mistral = get_flesch_score(df_llm_mistral, \"mistral\", \"aave_llm\")\n",
    "multi_flesch_score_mistral = get_flesch_score(df_multi_phonate_mistral, \"mistral\", \"aave_multivalue\")\n",
    "multi_phonate_flesch_score_mistral = get_flesch_score(df_multivalue_mistral, \"mistral\", \"aave_multi_phonate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('pickle/gpt4/flesch_score/sae_score.pkl', 'wb') as f:\n",
    "#     pickle.dump(reg_flesh_score_gpt4, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('pickle/gpt4/flesch_grade/sae_grade.pkl', 'wb') as f:\n",
    "#     pickle.dump(reg_flesch_grade_gpt4, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "mistral_path = \"pickle/mistral/\"\n",
    "gpt_sae_file = open(f'{mistral_path}flesch_score/sae_score.pkl', 'rb')\n",
    "mistral_sae_flesh_score  = pickle.load(gpt_sae_file)\n",
    "gpt_phoante_file = open(f'{mistral_path}flesch_score/aave_phonate_score.pkl', 'rb')\n",
    "mistral_phonate_flesh_score  = pickle.load(gpt_phoante_file)\n",
    "gpt_llm_file = open(f'{mistral_path}flesch_score/aave_llm_score.pkl', 'rb')\n",
    "mistral_llm_flesh_score  = pickle.load(gpt_llm_file)\n",
    "gpt_multi_phonate_file = open(f'{mistral_path}flesch_score/aave_multi_phonate_score.pkl', 'rb')\n",
    "mistral_multi_phonate_flesh_score  = pickle.load(gpt_multi_phonate_file)\n",
    "gpt_multivalue_file = open(f'{mistral_path}flesch_score/aave_multivalue_score.pkl', 'rb')\n",
    "mistral_multivalue_flesh_score  = pickle.load(gpt_multivalue_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "gpt4_path = \"pickle/gpt4/\"\n",
    "gpt_sae_file = open(f'{gpt4_path}flesch_score/sae_score.pkl', 'rb')\n",
    "gpt4_sae_flesh_score  = pickle.load(gpt_sae_file)\n",
    "gpt_phoante_file = open(f'{gpt4_path}flesch_score/aave_phonate_score.pkl', 'rb')\n",
    "gpt4_phonate_flesh_score  = pickle.load(gpt_phoante_file)\n",
    "gpt_llm_file = open(f'{gpt4_path}flesch_score/aave_llm_score.pkl', 'rb')\n",
    "gpt4_llm_flesh_score  = pickle.load(gpt_llm_file)\n",
    "gpt_multi_phonate_file = open(f'{gpt4_path}flesch_score/aave_multi_phonate_score.pkl', 'rb')\n",
    "gpt4_multi_phonate_flesh_score  = pickle.load(gpt_multi_phonate_file)\n",
    "gpt_multivalue_file = open(f'{gpt4_path}flesch_score/aave_multivalue_score.pkl', 'rb')\n",
    "gpt4_multivalue_flesh_score  = pickle.load(gpt_multivalue_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "gpt3_path = \"pickle/gpt3.5/\"\n",
    "gpt_sae_file = open(f'{gpt3_path}flesch_score/sae_score.pkl', 'rb')\n",
    "gpt3_sae_flesh_score  = pickle.load(gpt_sae_file)\n",
    "gpt_phoante_file = open(f'{gpt3_path}flesch_score/aave_phonate_score.pkl', 'rb')\n",
    "gpt3_phonate_flesh_score  = pickle.load(gpt_phoante_file)\n",
    "gpt_llm_file = open(f'{gpt3_path}flesch_score/aave_llm_score.pkl', 'rb')\n",
    "gpt3_llm_flesh_score  = pickle.load(gpt_llm_file)\n",
    "gpt_multi_phonate_file = open(f'{gpt3_path}flesch_score/aave_multi_phonate_score.pkl', 'rb')\n",
    "gpt3_multi_phonate_flesh_score  = pickle.load(gpt_multi_phonate_file)\n",
    "gpt_multivalue_file = open(f'{gpt3_path}flesch_score/aave_multivalue_score.pkl', 'rb')\n",
    "gpt3_multivalue_flesh_score  = pickle.load(gpt_multivalue_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "llama31_path = \"pickle/llama3.1/\"\n",
    "gpt_sae_file = open(f'{llama31_path}flesch_score/sae_score.pkl', 'rb')\n",
    "llama31_sae_flesh_score  = pickle.load(gpt_sae_file)\n",
    "gpt_phoante_file = open(f'{llama31_path}flesch_score/aave_phonate_score.pkl', 'rb')\n",
    "llama31_phonate_flesh_score  = pickle.load(gpt_phoante_file)\n",
    "gpt_llm_file = open(f'{llama31_path}flesch_score/aave_llm_score.pkl', 'rb')\n",
    "llama31_llm_flesh_score  = pickle.load(gpt_llm_file)\n",
    "gpt_multi_phonate_file = open(f'{llama31_path}flesch_score/aave_multi_phonate_score.pkl', 'rb')\n",
    "llama31_multi_phonate_flesh_score  = pickle.load(gpt_multi_phonate_file)\n",
    "gpt_multivalue_file = open(f'{llama31_path}flesch_score/aave_multivalue_score.pkl', 'rb')\n",
    "llama31_multivalue_flesh_score  = pickle.load(gpt_multivalue_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "llama32_path = \"pickle/llama3.2/\"\n",
    "gpt_sae_file = open(f'{llama32_path}flesch_score/sae_score.pkl', 'rb')\n",
    "llama32_sae_flesh_score  = pickle.load(gpt_sae_file)\n",
    "gpt_phoante_file = open(f'{llama32_path}flesch_score/aave_phonate_score.pkl', 'rb')\n",
    "llama32_phonate_flesh_score  = pickle.load(gpt_phoante_file)\n",
    "gpt_llm_file = open(f'{llama32_path}flesch_score/aave_llm_score.pkl', 'rb')\n",
    "llama32_llm_flesh_score  = pickle.load(gpt_llm_file)\n",
    "gpt_multi_phonate_file = open(f'{llama32_path}flesch_score/aave_multi_phonate_score.pkl', 'rb')\n",
    "llama32_multi_phonate_flesh_score  = pickle.load(gpt_multi_phonate_file)\n",
    "gpt_multivalue_file = open(f'{llama32_path}flesch_score/aave_multivalue_score.pkl', 'rb')\n",
    "llama32_multivalue_flesh_score  = pickle.load(gpt_multivalue_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "qwen_path = \"pickle/qwen2.5/\"\n",
    "gpt_sae_file = open(f'{qwen_path}flesch_score/sae_score.pkl', 'rb')\n",
    "qwen_sae_flesh_score  = pickle.load(gpt_sae_file)\n",
    "gpt_phoante_file = open(f'{qwen_path}flesch_score/aave_phonate_score.pkl', 'rb')\n",
    "qwen_phonate_flesh_score  = pickle.load(gpt_phoante_file)\n",
    "gpt_llm_file = open(f'{qwen_path}flesch_score/aave_llm_score.pkl', 'rb')\n",
    "qwen_llm_flesh_score  = pickle.load(gpt_llm_file)\n",
    "gpt_multi_phonate_file = open(f'{qwen_path}flesch_score/aave_multi_phonate_score.pkl', 'rb')\n",
    "qwen_multi_phonate_flesh_score  = pickle.load(gpt_multi_phonate_file)\n",
    "gpt_multivalue_file = open(f'{qwen_path}flesch_score/aave_multivalue_score.pkl', 'rb')\n",
    "qwen_multivalue_flesh_score  = pickle.load(gpt_multivalue_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "gemma_path = \"pickle/gemma2/\"\n",
    "gpt_sae_file = open(f'{gemma_path}flesch_score/sae_score.pkl', 'rb')\n",
    "gemma_sae_flesh_score  = pickle.load(gpt_sae_file)\n",
    "gpt_phoante_file = open(f'{gemma_path}flesch_score/aave_phonate_score.pkl', 'rb')\n",
    "gemma_phonate_flesh_score  = pickle.load(gpt_phoante_file)\n",
    "gpt_llm_file = open(f'{gemma_path}flesch_score/aave_llm_score.pkl', 'rb')\n",
    "gemma_llm_flesh_score  = pickle.load(gpt_llm_file)\n",
    "gpt_multi_phonate_file = open(f'{gemma_path}flesch_score/aave_multi_phonate_score.pkl', 'rb')\n",
    "gemma_multi_phonate_flesh_score  = pickle.load(gpt_multi_phonate_file)\n",
    "gpt_multivalue_file = open(f'{gemma_path}flesch_score/aave_multivalue_score.pkl', 'rb')\n",
    "gemma_multivalue_flesh_score  = pickle.load(gpt_multivalue_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def categorize_score(score):\n",
    "    if 90.0 <= score <= 100.0:\n",
    "        return \"5th grade\"\n",
    "    elif 80.0 <= score < 90.0:\n",
    "        return \"6th grade\"\n",
    "    elif 70.0 <= score < 80.0:\n",
    "        return \"7th grade\"\n",
    "    elif 60.0 <= score < 70.0:\n",
    "        return \"8th & 9th grade\"\n",
    "    elif 50.0 <= score < 60.0:\n",
    "        return \"10th to 12th grade\"\n",
    "    elif 30.0 <= score < 50.0:\n",
    "        return \"College\"\n",
    "    elif 10.0 <= score < 30.0:\n",
    "        return \"College graduate\"\n",
    "    elif 0.0 <= score < 10.0:\n",
    "        return \"Professional\"\n",
    "    else:\n",
    "        return \"Invalid Score\"\n",
    "\n",
    "gpt4_school_level_reg =  [categorize_score(i) for i in gpt4_sae_flesh_score] \n",
    "gpt4_school_level_llm =  [categorize_score(i) for i in gpt4_llm_flesh_score] \n",
    "gpt4_school_level_phonate = [categorize_score(i) for i in gpt4_phonate_flesh_score] \n",
    "gpt4_school_level_multi_value = [categorize_score(i) for i in gpt4_multivalue_flesh_score] \n",
    "gpt4_school_level_mutli_phonate = [categorize_score(i) for i in gpt4_multi_phonate_flesh_score] \n",
    "\n",
    "gpt3_school_level_reg = [categorize_score(i) for i in gpt3_sae_flesh_score] \n",
    "gpt3_school_level_llm = [categorize_score(i) for i in gpt3_llm_flesh_score] \n",
    "gpt3_school_level_phonate = [categorize_score(i) for i in gpt3_phonate_flesh_score] \n",
    "gpt3_school_level_multi_value = [categorize_score(i) for i in gpt3_multivalue_flesh_score] \n",
    "gpt3_school_level_mutli_phonate = [categorize_score(i) for i in gpt3_multi_phonate_flesh_score]\n",
    "\n",
    "llama31_school_level_reg = [categorize_score(i) for i in llama31_sae_flesh_score] \n",
    "llama31_school_level_llm = [categorize_score(i) for i in llama31_llm_flesh_score] \n",
    "llama31_school_level_phonate = [categorize_score(i) for i in llama31_phonate_flesh_score] \n",
    "llama31_school_level_multi_value = [categorize_score(i) for i in llama31_multivalue_flesh_score] \n",
    "llama31_school_level_mutli_phonate = [categorize_score(i) for i in llama31_multi_phonate_flesh_score] \n",
    "\n",
    "llama32_school_level_reg = [categorize_score(i) for i in llama32_sae_flesh_score] \n",
    "llama32_school_level_llm = [categorize_score(i) for i in llama32_llm_flesh_score] \n",
    "llama32_school_level_phonate = [categorize_score(i) for i in llama32_phonate_flesh_score] \n",
    "llama32_school_level_multi_value = [categorize_score(i) for i in llama32_multivalue_flesh_score] \n",
    "llama32_school_level_mutli_phonate = [categorize_score(i) for i in llama32_multi_phonate_flesh_score] \n",
    "\n",
    "qwen_school_level_reg = [categorize_score(i) for i in qwen_sae_flesh_score] \n",
    "qwen_school_level_llm = [categorize_score(i) for i in qwen_llm_flesh_score] \n",
    "qwen_school_level_phonate = [categorize_score(i) for i in qwen_phonate_flesh_score] \n",
    "qwen_school_level_multi_value = [categorize_score(i) for i in qwen_multivalue_flesh_score] \n",
    "qwen_school_level_mutli_phonate = [categorize_score(i) for i in qwen_multi_phonate_flesh_score] \n",
    "\n",
    "gemma_school_level_reg = [categorize_score(i) for i in gemma_sae_flesh_score] \n",
    "gemma_school_level_llm = [categorize_score(i) for i in gemma_llm_flesh_score] \n",
    "gemma_school_level_phonate = [categorize_score(i) for i in gemma_phonate_flesh_score] \n",
    "gemma_school_level_multi_value = [categorize_score(i) for i in gemma_multivalue_flesh_score] \n",
    "gemma_school_level_mutli_phonate = [categorize_score(i) for i in gemma_multi_phonate_flesh_score] \n",
    "\n",
    "mistral_school_level_reg = [categorize_score(i) for i in mistral_sae_flesh_score] \n",
    "mistral_school_level_llm = [categorize_score(i) for i in mistral_llm_flesh_score] \n",
    "mistral_school_level_phonate = [categorize_score(i) for i in mistral_phonate_flesh_score] \n",
    "mistral_school_level_multi_value = [categorize_score(i) for i in mistral_multivalue_flesh_score] \n",
    "mistral_school_level_mutli_phonate = [categorize_score(i) for i in mistral_multi_phonate_flesh_score] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, count = np.unique(np.array(mistral_school_level_reg), return_counts=True)\n",
    "key_aave, count_aave = np.unique(np.array(mistral_school_level_llm), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key)\n",
    "print(count)\n",
    "print(key_aave)\n",
    "print(count_aave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = []\n",
    "model.extend(['mistral']*16)\n",
    "# model.extend(['qwen2.5']*16)\n",
    "# model.extend(['llama3.2']*16)\n",
    "# model.extend(['llama3.1']*16)\n",
    "# model.extend(['gpt3.5']*16)\n",
    "# model.extend(['gpt4']*16)\n",
    "\n",
    "grade = [\"5th grade\",\"5th grade\",\"6th grade\",\"6th grade\", \"7th grade\", \"7th grade\",\"8th & 9th grade\",\"8th & 9th grade\", \"10th to 12th grade\", \"10th to 12th grade\",\"College\", \"College\", \"College graduate\",\"College graduate\",\"Professional\",\"Professional\"]\n",
    "question = [\"SAE Questions\", \"AAVE Questions\", \"SAE Questions\", \"AAVE Questions\", \"SAE Questions\", \"AAVE Questions\", \"SAE Questions\", \"AAVE Questions\",\"SAE Questions\", \"AAVE Questions\",\"SAE Questions\", \"AAVE Questions\",\"SAE Questions\", \"AAVE Questions\",\"SAE Questions\", \"AAVE Questions\"]\n",
    "\n",
    "frequency = []\n",
    "frequency.extend([1,1,17,26,76,114,196,248,351,476,1344,1445,777,485,62,21])\n",
    "# frequency.extend([11,11,65,82,163,187,240,375,459,691,1488,1324,415,176,8,3])\n",
    "# frequency.extend([5,6,34,77,121,236,247,541,495,697,1430,1051,486,172,16,3])\n",
    "# frequency.extend([2,13,35,91,150,376,318,640,658,745,1210,837,349,52,9,4])\n",
    "# frequency.extend([6,6,35,69,120,174,259,399,483,618,1124,1038,307,124,5,3])\n",
    "# frequency.extend([3,6,27,48,91,174,166,407,347, 627,1529, 1307,669,271,18,2])\n",
    "\n",
    "df_readability_new = pd.DataFrame(data = {'model':model, 'grade':grade, 'question':question, 'frequency':frequency})\n",
    "# df_readability = pd.DataFrame(data = {'model':model, 'grade':grade, 'question':question, 'frequency':frequency})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_readability_plot(df, model_name:str):\n",
    "    data = {\n",
    "        \"Grade Level\": df[df['model']==model_name]['grade'],\n",
    "        \"Question Dialect\": df[df['model']==model_name]['question'],\n",
    "        \"Frequency\": df[df['model']==model_name]['frequency'] # Notice one value is 0\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(11, 6))\n",
    "    ax = sns.barplot(\n",
    "        data=df,\n",
    "        x=\"Grade Level\",\n",
    "        y=\"Frequency\",\n",
    "        hue=\"Question Dialect\",\n",
    "        palette=\"muted\"\n",
    "    )\n",
    "    \n",
    "    # Adding values on top of the bars\n",
    "    for p in ax.patches:\n",
    "        if p.get_height() > 0:  # Only add text for bars with height > 0\n",
    "            ax.annotate(\n",
    "                f'{p.get_height():.0f}',  # Format the value as an integer\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),  # Position at bar center\n",
    "                ha='center',  # Horizontal alignment\n",
    "                va='center',  # Vertical alignment\n",
    "                xytext=(0, 8),  # Offset text position by 8 points\n",
    "                textcoords='offset points'\n",
    "            )\n",
    "    \n",
    "    # Adding labels\n",
    "    plt.title(f\"Distribution Flesch-Kincaid Readability Grade Level Equivalent of the Explanations({model_name})\", fontsize=16)\n",
    "    plt.xlabel(\"Grade Level\", fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.xticks(rotation = 25)\n",
    "    \n",
    "    # Show the plot\n",
    "    # plt.savefig(\"flesch_kincaid_readability_grade_gemma2.png\", dpi=300, format='png', bbox_inches='tight')\n",
    "    plt.legend(title=\"Question Dialect\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_readability_plot(df_readability, 'gemma2')\n",
    "# sae : 2782\n",
    "# aave : 2751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizerFast, XLMRobertaForSequenceClassification\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained('s-nlp/xlmr_formality_classifier')\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('s-nlp/xlmr_formality_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"s-nlp/roberta-base-formality-ranker\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"s-nlp/roberta-base-formality-ranker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lst = []\n",
    "hard_class = []\n",
    "for text in tqdm(list(df_regular_gpt3['answer'])):\n",
    "    # prepare the input\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=True,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    # inference\n",
    "    output = model(**encoding)\n",
    "    hard_class.append(output.logits.softmax(dim=1)[0].argmax().item())\n",
    "    probability = output.logits.softmax(dim=1)[0][0].item()\n",
    "    output_lst.append(probability)\n",
    "#0.9985365793161225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(hard_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[1-i for i in output_lst]\n",
    "plt.hist([1-i for i in output_lst],bins=20, range=(0, 1))\n",
    "plt.xticks(rotation = 45)\n",
    "plt.xlabel(\"probability\")\n",
    "plt.ylabel('frequency')\n",
    "plt.ylim((0,2300))\n",
    "plt.title(\"distribution of formality classifier probablity for sae answers\")\n",
    "plt.savefig(\"probability_distribution_formality_sae_roberta-base-formality-ranker.png\", dpi=300, format='png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lst_aave = []\n",
    "hard_class = []\n",
    "for text in tqdm(list(df_llm_gpt3['answer'])):\n",
    "    # prepare the input\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=True,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    # inference\n",
    "    output = model(**encoding)\n",
    "    probability = output.logits.softmax(dim=1)[0][0].item()\n",
    "    output_lst_aave.append(probability)\n",
    "## 0.9982370618900709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([1-i for i in output_lst_aave],bins=20,range=(0, 1))\n",
    "plt.xticks(rotation = 45)\n",
    "plt.xlabel(\"probability\")\n",
    "plt.ylabel('frequency')\n",
    "plt.ylim((0,2300))\n",
    "plt.title(\"distribution of formality classifier probablity for aave answers\")\n",
    "plt.savefig(\"probability_distribution_formality_aave_roberta-base-formality-ranker.png\", dpi=300, format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "informal = []\n",
    "for i in output_lst:\n",
    "    if i <0.5:\n",
    "        informal.append(i)\n",
    "print(len(informal))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "informal = []\n",
    "for i in output_lst_aave:\n",
    "    if i <0.5:\n",
    "        informal.append(i)\n",
    "print(len(informal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(output_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Converter\": [\"SAE\",\"SAE\",\"SAE\",\"AAVE_LLM\", \"AAVE_LLM\", \"AAVE_LLM\",],\n",
    "    \"Model\": [\"gpt3\", \"llama3.1\", \"gpt4\", \"gpt3\", \"llama3.1\", \"gpt4\", ],\n",
    "    \"Score\":[46.4,46.9,40.5,51.5,58.0,48.5,]  # Notice one value is 0\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.barplot(\n",
    "    data=df,\n",
    "    x=\"Converter\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Model\",\n",
    "    palette=\"muted\"\n",
    ")\n",
    "\n",
    "# Adding values on top of the bars\n",
    "for p in ax.patches:\n",
    "    if p.get_height() > 0:  # Only add text for bars with height > 0\n",
    "        ax.annotate(\n",
    "            f'{p.get_height():.0f}',  # Format the value as an integer\n",
    "            (p.get_x() + p.get_width() / 2., p.get_height()),  # Position at bar center\n",
    "            ha='center',  # Horizontal alignment\n",
    "            va='center',  # Vertical alignment\n",
    "            xytext=(0, 8),  # Offset text position by 8 points\n",
    "            textcoords='offset points'\n",
    "        )\n",
    "\n",
    "# Adding labels\n",
    "plt.title(\"Flesch-Kincaid Readability Score\", fontsize=16)\n",
    "plt.xlabel(\"Converter\", fontsize=12)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.ylim(0, 80)\n",
    "# Show the plot\n",
    "plt.savefig(\"flesch_kincaid_readability_score.png\", dpi=300, format='png', bbox_inches='tight')\n",
    "plt.legend(title=\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    \"Dialect\": [\"SAE\", \"SAE\", \"SAE\", \"AAVE_LLM\", \"AAVE_LLM\", \"AAVE_LLM\"],\n",
    "    \"Model\": [\"gpt3\", \"gpt4\", \"llama3.1\", \"gpt3\", \"gpt4\", \"llama3.1\"],\n",
    "    \"Accuracy\": [0.676, 0.826, 0.657, 0.602, 0.723, 0.530]  # Notice one value is 0\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.barplot(\n",
    "    data=df,\n",
    "    x=\"Dialect\",\n",
    "    y=\"Accuracy\",\n",
    "    hue=\"Model\",\n",
    "    palette=\"muted\"\n",
    ")\n",
    "\n",
    "# Adding values on top of the bars\n",
    "for p in ax.patches:\n",
    "    if p.get_height() > 0:  # Only add text for bars with height > 0\n",
    "        ax.annotate(\n",
    "            f'{p.get_height():.3f}',  # Format the value as an integer\n",
    "            (p.get_x() + p.get_width() / 2., p.get_height()),  # Position at bar center\n",
    "            ha='center',  # Horizontal alignment\n",
    "            va='center',  # Vertical alignment\n",
    "            xytext=(0, 8),  # Offset text position by 8 points\n",
    "            textcoords='offset points'\n",
    "        )\n",
    "\n",
    "# Adding labels\n",
    "plt.title(\"LLMs Performance on Questions of Different Dialects\", fontsize=16)\n",
    "plt.xlabel(\"Dialect\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(\"LLMs_performance_limited_model_partly.png\", dpi=300, format='png', bbox_inches='tight')\n",
    "plt.legend(title=\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \"AAVE_Phonate\", \"AAVE_Phonate\", \"AAVE_Phonate\", \"AAVE_Multi_Phonate\", \"AAVE_Multi_Phonate\", \"AAVE_Multi_Phonate\", \"AAVE_Multivalue\", \"AAVE_Multivalue\", \"AAVE_Multivalue\",\n",
    "#  \"gpt3\", \"gpt4\", \"llama3.1\", \"gpt3\", \"gpt4\", \"llama3.1\", \"gpt3\", \"gpt4\", \"llama3.1\"\n",
    "#  352, 601, 286, 406, 516, 293, 273, 361, 285,\n",
    "data = {\n",
    "    \"Converter\": [\"SAE\",\"SAE\",\"SAE\",\"AAVE_LLM\", \"AAVE_LLM\", \"AAVE_LLM\",\"AAVE_Phonate\", \"AAVE_Phonate\", \"AAVE_Phonate\", \"AAVE_Multi_Phonate\", \"AAVE_Multi_Phonate\", \"AAVE_Multi_Phonate\", \"AAVE_Multivalue\", \"AAVE_Multivalue\", \"AAVE_Multivalue\",],\n",
    "    \"Model\": [\"gpt3\", \"llama3.1\", \"gpt4\", \"gpt3\", \"llama3.1\", \"gpt4\", \"gpt3\", \"llama3.1\", \"gpt4\", \"gpt3\", \"llama3.1\", \"gpt4\", \"gpt3\", \"llama3.1\", \"gpt4\"],\n",
    "    \"Frequency\": [307, 248, 288, 337, 489, 357, 352, 601, 286, 406, 516, 293, 273, 361, 285,]  # Notice one value is 0\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.barplot(\n",
    "    data=df,\n",
    "    x=\"Converter\",\n",
    "    y=\"Frequency\",\n",
    "    hue=\"Model\",\n",
    "    palette=\"muted\"\n",
    ")\n",
    "\n",
    "# Adding values on top of the bars\n",
    "for p in ax.patches:\n",
    "    if p.get_height() > 0:  # Only add text for bars with height > 0\n",
    "        ax.annotate(\n",
    "            f'{p.get_height():.0f}',  # Format the value as an integer\n",
    "            (p.get_x() + p.get_width() / 2., p.get_height()),  # Position at bar center\n",
    "            ha='center',  # Horizontal alignment\n",
    "            va='center',  # Vertical alignment\n",
    "            xytext=(0, 8),  # Offset text position by 8 points\n",
    "            textcoords='offset points'\n",
    "        )\n",
    "\n",
    "# Adding labels\n",
    "plt.title(\"Number of Answers that Are Categorized as Polite\", fontsize=16)\n",
    "plt.xlabel(\"Converter\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(\"number_of_polite_answers_full.png\", dpi=300, format='png', bbox_inches='tight')\n",
    "plt.legend(title=\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"AAVE_Phonate\", \"AAVE_Phonate\", \"AAVE_Phonate\", \"AAVE_Multi_Phonate\", \"AAVE_Multi_Phonate\", \"AAVE_Multi_Phonate\", \"AAVE_Multivalue\", \"AAVE_Multivalue\", \"AAVE_Multivalue\",\n",
    "#\"gpt3\", \"llama3.1\", \"gpt4\", \"gpt3\", \"llama3.1\", \"gpt4\", \"gpt3\", \"llama3.1\", \"gpt4\"\n",
    "#2498,2249,2564,2444,2334,2557,2577,2489,2565\n",
    "data = {\n",
    "    \"Converter\": [\"SAE\",\"SAE\",\"SAE\",\"AAVE_LLM\", \"AAVE_LLM\", \"AAVE_LLM\",],\n",
    "    \"Model\": [\"gpt3\", \"llama3.1\", \"gpt4\", \"gpt3\", \"llama3.1\", \"gpt4\", ],\n",
    "    \"Frequency\":[2543,2602,2562,2513,2361,2493,]  # Notice one value is 0\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.barplot(\n",
    "    data=df,\n",
    "    x=\"Converter\",\n",
    "    y=\"Frequency\",\n",
    "    hue=\"Model\",\n",
    "    palette=\"muted\"\n",
    ")\n",
    "\n",
    "# Adding values on top of the bars\n",
    "for p in ax.patches:\n",
    "    if p.get_height() > 0:  # Only add text for bars with height > 0\n",
    "        ax.annotate(\n",
    "            f'{p.get_height():.0f}',  # Format the value as an integer\n",
    "            (p.get_x() + p.get_width() / 2., p.get_height()),  # Position at bar center\n",
    "            ha='center',  # Horizontal alignment\n",
    "            va='center',  # Vertical alignment\n",
    "            xytext=(0, 8),  # Offset text position by 8 points\n",
    "            textcoords='offset points'\n",
    "        )\n",
    "\n",
    "# Adding labels\n",
    "plt.title(\"Number of Answers that Are Categorized as Neutral\", fontsize=16)\n",
    "plt.xlabel(\"Converter\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.ylim(0, 3400)\n",
    "# Show the plot\n",
    "plt.savefig(\"number_of_neutral_answers_partly.png\", dpi=300, format='png', bbox_inches='tight')\n",
    "plt.legend(title=\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regular_gpt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regular_inst = pd.DataFrame(columns = df_regular_gpt3.columns)\n",
    "\n",
    "for i in range(len(df_regular_gpt3)):\n",
    "    if i %50 ==0 or i %50 ==1 or i %50 ==2 or i %50 ==3 or i %50 ==4:\n",
    "        df_regular_inst.loc[len(df_regular_inst)] = df_regular_gpt3.loc[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_regular_inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aave_regular_inst = pd.read_csv('aave_instruct.csv')\n",
    "aave_regular_inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_regular = (df_regular_inst['letter_answer'] == df_regular_inst['correct_answer']).sum()/len(df_regular_inst)\n",
    "matches_regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_aave= (aave_regular_inst['letter_answer'] == aave_regular_inst['correct_answer']).sum()/len(aave_regular_inst)\n",
    "match_aave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic Marker Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize(text):\n",
    "    # you may want to use a smarter tokenizer\n",
    "    for match in re.finditer(r'\\w+', text, re.UNICODE):\n",
    "        yield match.group(0)\n",
    "\n",
    "import liwc\n",
    "parse, category_names = liwc.load_token_parser('LIWC2007_English100131.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_counter_sae = Counter()\n",
    "token_num_sae = 0\n",
    "for i in range(len(df_regular_gpt3)):\n",
    "    answer  = df_regular_gpt3.loc[i]['answer']\n",
    "    count_tokens = tokenize(answer)\n",
    "    for tok in count_tokens:\n",
    "        token_num_sae+=1\n",
    "    answer_tokens = tokenize(answer)\n",
    "\n",
    "    # now flatmap over all the categories in all of the tokens using a generator:\n",
    "    answer_counts = Counter(category for token in answer_tokens for category in parse(token))\n",
    "    # and print the results:\n",
    "    merged_counter_sae +=answer_counts\n",
    "normalized_counter_sae = Counter({word: (count / token_num_sae)*1000 for word, count in merged_counter_sae.items()})\n",
    "\n",
    "print(merged_counter_sae)\n",
    "print(token_num_sae)\n",
    "print(normalized_counter_sae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_sae['posemo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_sae['negemo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_sae['certain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_sae['tentat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_sae['insight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_sae['cause']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_sae['percept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_sae['pronoun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_counter_aave = Counter()\n",
    "token_num_aave = 0\n",
    "for i in range(len(df_llm_gpt3)):\n",
    "    answer  = df_llm_gpt3.loc[i]['answer']\n",
    "    count_tokens = tokenize(answer)\n",
    "    for tok in count_tokens:\n",
    "        token_num_aave+=1\n",
    "    answer_tokens = tokenize(answer)\n",
    "\n",
    "    # now flatmap over all the categories in all of the tokens using a generator:\n",
    "    answer_counts = Counter(category for token in answer_tokens for category in parse(token))\n",
    "    # and print the results:\n",
    "    merged_counter_aave +=answer_counts\n",
    "normalized_counter_aave= Counter({word: (count / token_num_sae)*1000 for word, count in merged_counter_aave.items()})\n",
    "\n",
    "print(merged_counter_aave)\n",
    "print(token_num_aave)\n",
    "print(normalized_counter_aave)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_aave['posemo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_aave['negemo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_aave['certain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_aave['tentat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_aave['insight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_aave['cause']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_aave['percept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counter_aave['pronoun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
